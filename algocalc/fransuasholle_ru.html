<!--  -*- mode: html ; coding: utf-8  -*- ---------------------------------- -->
<html>

<head>

<title>Algo Calc</title>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="robots" content="all">
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="">

</head>

<center>
<table width=95%>
<tr><td>

<!-- --------------------------TOP------------------------------------- -->
<table width=100% border=2 cellspacing=0 cellpadding=5>
<tr align=center>
  <td width=15% align="center" bgcolor="cyan"> 
     <a href="./index.html"><b>HOME</b></a> <br>
  <td bgcolor="blue" >
    <font color="yellow"><b>Algo Calc </b></font>
    <img src="img/em.png" align="right"> 
  <td width=15% align="center" bgcolor="cyan">  
     <a href="./fransuasholle_ru.html"><b>RU</b></a> &nbsp; , &nbsp;  
     <a href="./fransuasholle_en.html"><b>EN</b></a> <br>
</table>
<!-- ------------------------- TOP------------------------------------- -->

<br clear=all>
<br clear=all>
<p><p>

<b><center>	  
    <font size=+2 color="red">
Фрагмент из книги Франсуа Шолле "Глубокое обучение на Phyton". <br>
    </font>
<br>

&copy; 2018, Manning, &copy; 2018, Питер

<br clear=all>
<a href="https://www.piter.com/collection/all/product/glubokoe-obuchenie-na-python">
  книга на www.piter.com </a>

</center></b> 

                                                              
<br clear=all>

<hr> <!-- ------------------------------------------------------ -->


<h3>СОДЕРЖАНИЕ</h3> 

<ol><b>
    <a href="#met93"> 9.3  БУДУЩЕЕ ГЛУБОКОГО ОБУЧЕНИЯ</a> <br><br>
    
    <a href="#met931">9.3.1 Модели как программы    </a>  <br>
    <a href="#met934">9.3.4 Непрерывное обучение и повторное использование
модульных подпрограмм</a>  <br>
    <a href="#met935">9.3.5 Долгосрочная перспектива</a> <br>

</b></ol>

<br clear=all>
<hr> <!-- ------------------------------------------------------ -->
   
  <a name="met93"></a><h3> 9.3. БУДУЩЕЕ ГЛУБОКОГО ОБУЧЕНИЯ
  </h3>
    
<p align="justify">


Это самый гипотетический раздел, цель которого — расширить горизонты для тех,
кто желает присоединиться к существующей исследовательской программе или
заняться своими исследованиями. Зная, как действуют глубокие сети, и понимая
текущее положение дел в сфере исследований, можем ли мы предсказать направление
движения в среднесрочной перспективе? Далее приводятся некоторые мои
личные мысли. Имейте в виду, что у меня нет хрустального шара, поэтому многим
моим ожиданиям, может быть, не суждено стать реальностью. Я разделяю эти прогнозы
не потому, что их состоятельность будет доказана в ближайшем будущем,
а потому, что они интересны и выглядят реальными в настоящем.

<p align="justify">
 
Вот основные направления, которые мне кажутся многообещающими:

<p align="justify">
 
<menu>

  <li> <i>Модели, более близкие к универсальным компьютерным программам</i>, построенные
    на основе более широкого набора примитивов, чем современные дифференцируемые
    слои. Именно так мы приблизимся к возможности моделирования
    рассуждений и обобщения, отсутствие которой является основным недостатком
    современных моделей. <br clear=all><br>
  
  <li> <i>Новые формы обучения, делающие возможным предыдущий пункт</i>, которые
    позволят моделям отойти от дифференцируемых преобразований. <br clear=all><br>
  
  <li> <i>Модели, требующие меньше участия людей-ииженеров</i>. Это не ваша задача —
      бесконечно крутить ручки настройки. <br clear=all><br>
  
  <li> <i>Расширение систематического повторного использования прежде извлеченных
      признаков и сконструированных архитектур</i>, с созданием систем метаобучения,
    использующих модульные подпрограммы.

</menu>

<p align="justify">
 
Обратите внимание: эти соображения не относятся к какому-то конкретному
виду контролируемого обучения, которое до сих пор остается хлебом насущным
глубокого обучения, скорее они применимы к любой форме машинного обучения,
включая неконтролируемое и самоконтролируемое обучение, а также обучение
с подкреплением. Принципиально не важно, откуда берутся размеченные данные
или как выглядит цикл обучения; это всего лишь разные ветви машинного обучения
— разные грани одной и той же конструкции. Давайте рассмотрим их поближе.

<p>
	
  <a name="met931"></a><h3>9.3.1. Модели как программы
  </h3>

<p align="justify">

Как отмечалось в предыдущем разделе, одна из обязательных трансформаций
в сфере машинного обучения, которые мы можем ожидать, — это уход от моделей,
реализующих лишь распознавание шаблонов и способных только на локальные
обобщения, в сторону моделей, способных абстрагировать и рассуждать и тем
самым достигать экстремального обобщения. Все современные программы ИИ, способные
на простейшие рассуждения, написаны человеком-программистом: например, программы,
опирающиеся на алгоритмы поиска, манипулирование графами
и формальную логику. В игре AlphaGo компании DeepMind, например, большая
часть интеллекта спроектирована и запрограммирована опытными программистами с
применением четких алгоритмов (таких, как алгоритм Монте-Карло для
поиска в деревьях); обучение на данных происходит только в специализированных
модулях (оценочные и стратегические сети). Однако в будущем такие системы ИИ
могут стать полностью обучаемыми без участия человека.

<p align="justify">

  Как такое может произойти? Рассмотрим хорошо изученный тип сетей: рекуррентные
  нейронные сети (RNN). Важно отметить, что RNN имеют немного
меньше ограничений, чем сети прямого распространения, потому что RNN — это
  чуть больше, чем простые геометрические преобразования: это геометрические
  преобразования, многократно повторяемые во внутреннем цикле for. Сам
временной цикл for «зашит» человеком-разработчиком: это предположение,
  имплантированное в сеть. Естественно, рекуррентные сети все еще очень ограничены
  в возможности представления, в первую очередь потому, что каждый
их шаг является дифференцируемым геометрическим преобразованием, и они
  переносят информацию из шага в шаг через точки в непрерывном геометрическом
  пространстве (векторы состояний). Теперь вообразите нейронную сеть,
дополненную программными примитивами, но вместо единственного жестко
зашитого цикла for с четко определенной геометрической памятью она включает
в себя обширный набор программных примитивов, которыми может свободно
манипулировать и расширять свои функции обработки, организуя ветвление
  с помощью инструкции if, выполняя условные циклы while, создавая переменные,
  используя диск в качестве долговременного хранилища, применяя операции
сортировки, используя сложные структуры данных (например, списки, графы
и хеш-таблицы) и многое другое. Пространство программ, которые такая сеть
сможет представить, было бы намного шире, чем то, которое можно представить
  с помощью современных моделей глубокого обучения, и некоторые из этих программ
  могли бы достигать высочайшей степени обобщения.

<p align="justify">

  Мы одновременно уйдем от жестко запрограммированного интеллекта (программного
  обеспечения, написанного вручную) и от обучаемого геометрического
интеллекта (глубокое обучение). Вместо этого мы получим сочетание формальных
алгоритмических, поддерживающих возможность абстрагирования и рассуждения
  модулей и геометрических модулей, поддерживающих неформальное знание и распознавание
  шаблонов. Вся система будет обучаться без участия или с минимальным
  участием человека.

<p align="justify">

Родственная подобласть ИИ, которая, как мне кажется, может «взлететь», — это
синтез программ, в частности синтез нейронных программ. Синтез программ заключается
в создании простых программ с использованием алгоритма поиска
(возможно, генетического поиска, как в генетическом программировании) для
исследования обширного пространства возможных программ. Поиск останавливается
при обнаружении программы, соответствующей заданным требованиям, часто
имеющим форму множества пар ввод/вывод. Это очень напоминает машинное обучение:
по заданным обучающим данным, имеющим форму пар ввод/вывод, мы находим программу,
которая соответствует входным и выходным данным и способна
обобщать новые входные данные. Различие в том, что вместо обучения значений
параметров в четко определенной программе (нейронной сети) мы генерируем
исходный код посредством процесса дискретного поиска.

<p align="justify">

Я определенно ожидаю увидеть в этой области новую волну интереса в ближайшие
несколько лет. В частности, я ожидаю появления новой смежной области между
глубоким обучением и синтезом программ, где вместо программ на языке общего
назначения будут генерироваться нейронные сети (потоки геометрической обработки
данных), дополненные широким набором алгоритмических примитивов,
таких как циклы for, и многих других (рис. 9 .5). Это должно быть более практично
и полезно, чем прямой синтез исходного кода, и существенно расширит диапазон
задач, поддающихся решению с применением машинного обучения, — пространство
программ, которые можно автоматически генерировать на основе обучающих данных.
Современные рекуррентные сети можно считать предтечами таких гибридных
алгоритмически-геометрических моделей.


<center>
<!-- <img src="img/image003.png" width="600" height="400"> -->
<br clear=all><br>
<b>Рисунок 9.5 – Программа, сгенерированная одновременно на основе геометрических
  (распознавание шаблонов, предсказание) и алгоритмических (рассуждения, поиск,
  память) примитивов.</b>
</center>


<br>
  
<a name="met934"></a><h3>9.3.4. Непрерывное обучение и повторное использование
модульных подпрограмм
</h3>

<p align="justify">
  
  Когда модели станут сложнее и будут основаны на более насыщенных алгоритмических
  примитивах, эта повышенная сложность потребует увеличить степень
повторного использования результатов прежних решений вместо обучения новых
моделей с нуля каждый раз, когда возникает новая задача или новый набор данных.
  Многие наборы данных содержат недостаточно информации, чтобы мы могли приступить
  к созданию новых, сложных моделей с нуля, и поэтому необходимо будет
использовать информацию из прежних наборов данных (представьте, что вам
пришлось бы изучать русский язык с нуля всякий раз, когда вы открываете новую
книгу, — это было бы просто невозможно). Обучение моделей с нуля для каждой
новой задачи неэффективно также из-за большого перекрытия между текущими
задачами и прежними.

<p align="justify">

  В последние годы неоднократно отмечалось интересное наблюдение: обучение
  одной и той же модели для решения мало связанных между собой задач дает в результате
  модель, которая лучше подходит для каждой задачи. Например, обучение
одной и той же нейронной модели машинного перевода с английского на немецкий
и с французского на итальянский дает в результате модель, которая лучше подходит
  для каждой пары языков. Аналогично, одновременное обучение модели классификации
  и сегментации изображений с использованием одной и той же сверточной
основы дает в результате модель, которая лучше решает обе задачи. Это вполне
  объяснимо: в малосвязанных задачах всегда какая-то часть информации является
  общей, в результате объединенная модель получает доступ к большему объему
информации о каждой отдельной задаче, нежели модель, обучаемая для решения
какой-то конкретной задачи.

<p align="justify">

  В настоящее время под повторным использованием моделей в разных задачах
  подразумевается использование обученных весов моделей, выполняющих универсальные
  функции, такие как выделение визуальных признаков. Пример этого вы видели
в главе 5. В будущем я ожидаю, что в обиход войдет более обобщенная версия: мы
будем использовать не только ранее извлеченные признаки (веса подмоделей), но
также архитектуры моделей и процедуры обучения. По мере того как модели будут
становиться все более похожими на программы, мы начнем повторно использовать
подпрограммы подобно классам и функциям в обычных языках программирования.

<p align="justify">

Представьте современный процесс разработки программного обеспечения: решив
определенную задачу (например, поддержку HTTP-запросов в Python), инженер
тут же упаковывает решение в абстрактную библиотеку многократного пользования.
Другие инженеры, столкнувшись с подобной проблемой в будущем, смогут
отыскать существующие библиотеки, загрузить их и использовать в своих проектах.
Похожим способом в будущем системы метаобучения смогут собирать новые программы,
просеивая глобальную библиотеку высокоуровневых блоков многократного пользования.
Когда система обнаружит, что ей нужны схожие подпрограммы для
нескольких разных задач, она сможет создать абстрактную многоразовую версию
подпрограммы и сохранить ее в глобальной библиотеке (рис. 9 .6). Такой процесс
реализует абстракцию: необходимый компонент для достижения экстремального
обобщения. О подпрограмме, полезной для решения различных задач в разных
областях, можно сказать, что она абстрактная в отношении некоторых аспектов
решаемой задачи. Это определение абстракции похоже на понятие абстракции
в разработке программного обеспечения. Такие подпрограммы могут быть геометрическими
(модули глубокого обучения с предварительно выделенными представлениями)
или алгоритмическими (ближе к библиотекам, которыми пользуются
современные программисты).


<center>
<!-- <img src="img/image009.png" width="500" height="400"> -->
<br clear=all><br>
<b>
  Рисунок 9.6 – Система метаобучения, способная быстро разрабатывать модели для
  конкретных задач, используя примитивы многократного пользования (алгоритмические
  и геометрические), и таким способом достигать экстремального обобщения.
</b>
</center>

<br>
  
  <a name="met935"></a><h3> 9.3.5. Долгосрочная перспектива
  </h3>

<p align="justify">

Вот какой я вижу долгосрочную перспективу машинного обучения:

<p align="justify">

<menu>

<li> Модели будут больше похожи на программы и будут обладать возможностями,
выходящими далеко за рамки непрерывных геометрических преобразований
входных данных, которые мы используем в настоящее время. Эти программы,
  вероятно, будут ближе к абстрактным ментальным моделям, которые люди выстраивают
  в своем сознании, и будут способны к более широкому обобщению
благодаря богатой алгоритмической природе.  <br clear=all><br>

<li> Модели будут сочетать в себе алгоритмические модули, реализующие возможность
  формальных рассуждений, поиск и средства абстрагирования с геометрическими
  модулями, обеспечивающими неформальное знание и распознавание
шаблонов. AlphaGo (система, для создания которой потребовалось программное
обеспечение, созданное вручную, и множество решений, принятых людьми)
  являет собой ранний пример того, как может выглядеть такое сочетание символического
  и геометрического ИИ.  <br clear=all><br>

<li> Такие модели будут создаваться автоматически, без участия людей-инженеров,
  с использованием модульных компонентов, хранящихся в глобальной библиотеке
  подпрограмм многократного пользования — библиотеке, накапливающей
высококачественные модели, обученные ранее на тысячах задач и наборов
  данных. Часто встречающиеся шаблоны решений задач будут идентифицироваться
  системой метаобучения, превращаться в подпрограммы многократного
  пользования — подобно функциям и классам в разработке программного обеспечения
  — и добавляться в глобальную библиотеку. Это приведет к абстракции.  <br clear=all><br>

<li> Эта глобальная библиотека и связанная с ней система моделей смогут достичь
уровня экстремального обобщения, сопоставимого с человеческим: для новой
задачи или ситуации система сможет сконструировать новую работающую
  модель, использовав очень небольшой объем данных, благодаря широте программных
  примитивов, поддерживающих обобщение, и обширному опыту
решения похожих задач. Точно так же люди быстро осваивают новую сложную
видеоигру, опираясь на прежний опыт использования других видеоигр, а не
основываются на простом отображении стимулов в действия. Так происходит
потому, что модели, сформированные на базе предыдущего опыта, являются
абстрактными и похожими на программы. <br clear=all><br>

<li> Такую непрерывно развивающуюся систему моделей можно рассматривать как
общий искусственный интеллект (Artificial General Intelligence, AGI). Однако не
  нужно ожидать, что в результате возникнет какой-то необычный апокалиптический робот:
  это чистая фантазия, порожденная длинной последовательностью
глубоких недоразумений и непонимания как интеллекта, так и технологий.
Впрочем, такая критика не является целью данной книги. 


</menu>


<!-- ---------------------------BOTTOM---------------------------------- -->
<br clear=all><br>
<table width=100% border=2 cellspacing=0 cellpadding=5>
<tr align=center>
  <td width=15% align="center" bgcolor="cyan"> 
     <!-- <a href="../../README/doc/indru.html"><b>TO UP</b></a> -->
  <td align="center" bgcolor="blue"> 
     <b><font color="yellow">Algo Calc &nbsp; ( </font> 
     <font color="yellow" size=-1>
<!-- hhmts start -->
Last modified: Sat Feb 16 22:26:47 MSK 2019
<!-- hhmts end -->
     </font><font color="yellow">)</font></b>
     <td width=15% align="center" bgcolor="cyan">  
     <!-- <a href="../../README/doc/indru.html"><b>HOME</b></a> <br> -->
</table>
<!-- ---------------------------BOTTOM---------------------------------- -->


</td></tr>
</table>
</center>


</body>
</html>
